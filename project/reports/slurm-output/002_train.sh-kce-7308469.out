wandb: Currently logged in as: kenevoldsen. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.13.3
wandb: Run data is saved locally in /faststorage/project/NLPPred/github/snip/wandb/run-20221002_025630-3qap02dh
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run desert-feather-24
wandb: ‚≠êÔ∏è View project at https://wandb.ai/kenevoldsen/local%20mlp
wandb: üöÄ View run at https://wandb.ai/kenevoldsen/local%20mlp/runs/3qap02dh
/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:352: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.
  rank_zero_warn(
[38;5;4m‚Ñπ Training 19 models.[0m
Using 16bit native Automatic Mixed Precision (AMP)
/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:57: LightningDeprecationWarning: Setting `Trainer(weights_save_path=)` has been deprecated in v1.6 and will be removed in v1.8. Please pass ``dirpath`` directly to the `ModelCheckpoint` callback
  rank_zero_deprecation(
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
[38;5;4m‚Ñπ Training model 0[0m
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
Finding best initial lr:   0%|          | 0/100 [00:00<?, ?it/s]Finding best initial lr:   3%|‚ñé         | 3/100 [00:00<00:03, 29.30it/s]Finding best initial lr:   6%|‚ñå         | 6/100 [00:00<00:09,  9.77it/s]Finding best initial lr:   8%|‚ñä         | 8/100 [00:01<00:14,  6.45it/s]Finding best initial lr:  12%|‚ñà‚ñè        | 12/100 [00:02<00:20,  4.25it/s]Finding best initial lr:  16%|‚ñà‚ñå        | 16/100 [00:02<00:16,  5.00it/s]Finding best initial lr:  20%|‚ñà‚ñà        | 20/100 [00:03<00:14,  5.62it/s]Finding best initial lr:  23%|‚ñà‚ñà‚ñé       | 23/100 [00:03<00:10,  7.23it/s]Finding best initial lr:  25%|‚ñà‚ñà‚ñå       | 25/100 [00:04<00:12,  6.09it/s]Finding best initial lr:  28%|‚ñà‚ñà‚ñä       | 28/100 [00:05<00:16,  4.50it/s]Finding best initial lr:  32%|‚ñà‚ñà‚ñà‚ñè      | 32/100 [00:05<00:13,  5.18it/s]Finding best initial lr:  34%|‚ñà‚ñà‚ñà‚ñç      | 34/100 [00:05<00:10,  6.08it/s]Finding best initial lr:  36%|‚ñà‚ñà‚ñà‚ñå      | 36/100 [00:06<00:11,  5.75it/s]Finding best initial lr:  37%|‚ñà‚ñà‚ñà‚ñã      | 37/100 [00:06<00:10,  6.04it/s]Finding best initial lr:  40%|‚ñà‚ñà‚ñà‚ñà      | 40/100 [00:06<00:09,  6.44it/s]Finding best initial lr:  41%|‚ñà‚ñà‚ñà‚ñà      | 41/100 [00:06<00:08,  6.64it/s]Finding best initial lr:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 44/100 [00:08<00:14,  4.00it/s]Finding best initial lr:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 48/100 [00:08<00:10,  4.82it/s]Finding best initial lr:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 52/100 [00:09<00:08,  5.47it/s]Finding best initial lr:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 53/100 [00:09<00:08,  5.57it/s]Finding best initial lr:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 56/100 [00:09<00:07,  6.09it/s]Finding best initial lr:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 57/100 [00:10<00:07,  5.85it/s]Finding best initial lr:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 60/100 [00:11<00:08,  4.47it/s]Finding best initial lr:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 64/100 [00:11<00:06,  5.37it/s]Finding best initial lr:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 65/100 [00:11<00:06,  5.59it/s]Finding best initial lr:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 68/100 [00:12<00:05,  5.71it/s]Finding best initial lr:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 72/100 [00:12<00:04,  6.41it/s]Finding best initial lr:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 75/100 [00:12<00:02,  8.37it/s]Finding best initial lr:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 77/100 [00:13<00:04,  5.24it/s]Finding best initial lr:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 80/100 [00:14<00:03,  5.27it/s]Finding best initial lr:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 83/100 [00:14<00:02,  7.12it/s]Finding best initial lr:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 85/100 [00:14<00:02,  6.15it/s]Finding best initial lr:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 87/100 [00:14<00:01,  7.22it/s]Finding best initial lr:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 89/100 [00:15<00:01,  6.52it/s]Finding best initial lr:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 91/100 [00:15<00:01,  7.74it/s]Finding best initial lr:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 93/100 [00:16<00:01,  4.31it/s]Finding best initial lr:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 96/100 [00:17<00:00,  4.61it/s]Finding best initial lr: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:17<00:00,  5.40it/s]`Trainer.fit` stopped: `max_steps=100` reached.
Finding best initial lr: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100/100 [00:18<00:00,  5.48it/s]
Restoring states from the checkpoint path at models/.lr_find_03599f60-c896-4d24-8d78-efc9afac8f1d.ckpt
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name    | Type    | Params
------------------------------------
0 | encoder | MLP     | 230 K 
1 | decoder | MLP     | 231 K 
2 | loss    | MSELoss | 0     
------------------------------------
462 K     Trainable params
0         Non-trainable params
462 K     Total params
0.924     Total estimated model params size (MB)
Training: 0it [00:00, ?it/s]Training: 0it [00:00, ?it/s]Epoch 0: : 0it [00:00, ?it/s]Epoch 0: : 1it [00:01,  1.18s/it]Epoch 0: : 1it [00:01,  1.21s/it, loss=1.94, v_num=02dh]Epoch 0: : 2it [00:01,  1.63it/s, loss=1.94, v_num=02dh]Epoch 0: : 2it [00:01,  1.63it/s, loss=1.82, v_num=02dh]Epoch 0: : 3it [00:01,  2.41it/s, loss=1.82, v_num=02dh]Epoch 0: : 3it [00:01,  2.41it/s, loss=1.69, v_num=02dh]Epoch 0: : 4it [00:01,  2.46it/s, loss=1.69, v_num=02dh]Epoch 0: : 4it [00:01,  2.45it/s, loss=1.52, v_num=02dh]Epoch 0: : 5it [00:01,  2.95it/s, loss=1.52, v_num=02dh]Epoch 0: : 5it [00:01,  2.95it/s, loss=1.43, v_num=02dh]Epoch 0: : 6it [00:01,  3.49it/s, loss=1.43, v_num=02dh]Epoch 0: : 6it [00:01,  3.49it/s, loss=1.34, v_num=02dh]Epoch 0: : 7it [00:01,  4.03it/s, loss=1.34, v_num=02dh]Epoch 0: : 7it [00:01,  4.03it/s, loss=1.28, v_num=02dh]Epoch 0: : 8it [00:02,  3.64it/s, loss=1.28, v_num=02dh]Epoch 0: : 8it [00:02,  3.64it/s, loss=1.28, v_num=02dh]Epoch 0: : 9it [00:02,  4.00it/s, loss=1.28, v_num=02dh]Epoch 0: : 9it [00:02,  4.00it/s, loss=1.23, v_num=02dh]Epoch 0: : 10it [00:02,  4.27it/s, loss=1.23, v_num=02dh]Epoch 0: : 10it [00:02,  4.27it/s, loss=1.21, v_num=02dh]Epoch 0: : 11it [00:02,  4.66it/s, loss=1.21, v_num=02dh]Epoch 0: : 11it [00:02,  4.66it/s, loss=1.17, v_num=02dh]Epoch 0: : 12it [00:02,  4.25it/s, loss=1.17, v_num=02dh]Epoch 0: : 12it [00:02,  4.25it/s, loss=1.15, v_num=02dh]Epoch 0: : 13it [00:02,  4.56it/s, loss=1.15, v_num=02dh]Epoch 0: : 13it [00:02,  4.56it/s, loss=1.13, v_num=02dh]Epoch 0: : 14it [00:02,  4.82it/s, loss=1.13, v_num=02dh]Epoch 0: : 14it [00:02,  4.82it/s, loss=1.12, v_num=02dh]Epoch 0: : 15it [00:02,  5.14it/s, loss=1.12, v_num=02dh]Epoch 0: : 15it [00:02,  5.14it/s, loss=1.13, v_num=02dh]Epoch 0: : 16it [00:03,  4.64it/s, loss=1.13, v_num=02dh]Epoch 0: : 16it [00:03,  4.63it/s, loss=1.1, v_num=02dh] Epoch 0: : 17it [00:03,  4.90it/s, loss=1.1, v_num=02dh]Epoch 0: : 17it [00:03,  4.90it/s, loss=1.1, v_num=02dh]Epoch 0: : 18it [00:03,  5.10it/s, loss=1.1, v_num=02dh]Epoch 0: : 18it [00:03,  5.10it/s, loss=1.1, v_num=02dh]Epoch 0: : 19it [00:03,  5.36it/s, loss=1.1, v_num=02dh]Epoch 0: : 19it [00:03,  5.35it/s, loss=1.11, v_num=02dh]Epoch 0: : 20it [00:03,  5.03it/s, loss=1.11, v_num=02dh]Epoch 0: : 20it [00:03,  5.03it/s, loss=1.11, v_num=02dh]Epoch 0: : 21it [00:03,  5.26it/s, loss=1.11, v_num=02dh]Epoch 0: : 21it [00:03,  5.25it/s, loss=0.998, v_num=02dh]Epoch 0: : 22it [00:04,  5.44it/s, loss=0.998, v_num=02dh]Epoch 0: : 22it [00:04,  5.43it/s, loss=0.93, v_num=02dh] Epoch 0: : 23it [00:04,  5.65it/s, loss=0.93, v_num=02dh]Epoch 0: : 23it [00:04,  5.65it/s, loss=0.874, v_num=02dh]Epoch 0: : 24it [00:04,  5.35it/s, loss=0.874, v_num=02dh]Epoch 0: : 24it [00:04,  5.34it/s, loss=0.855, v_num=02dh]Epoch 0: : 25it [00:04,  5.54it/s, loss=0.855, v_num=02dh]Epoch 0: : 25it [00:04,  5.54it/s, loss=0.837, v_num=02dh]Epoch 0: : 26it [00:04,  5.69it/s, loss=0.837, v_num=02dh]Epoch 0: : 26it [00:04,  5.69it/s, loss=0.824, v_num=02dh]Epoch 0: : 27it [00:04,  5.88it/s, loss=0.824, v_num=02dh]Epoch 0: : 27it [00:04,  5.88it/s, loss=0.81, v_num=02dh] Epoch 0: : 28it [00:05,  5.59it/s, loss=0.81, v_num=02dh]Epoch 0: : 28it [00:05,  5.59it/s, loss=0.796, v_num=02dh]Epoch 0: : 29it [00:05,  5.77it/s, loss=0.796, v_num=02dh]Epoch 0: : 29it [00:05,  5.77it/s, loss=0.786, v_num=02dh]Epoch 0: : 30it [00:05,  5.90it/s, loss=0.786, v_num=02dh]Epoch 0: : 30it [00:05,  5.90it/s, loss=0.777, v_num=02dh]Epoch 0: : 31it [00:05,  6.07it/s, loss=0.777, v_num=02dh]Epoch 0: : 31it [00:05,  6.07it/s, loss=0.769, v_num=02dh]Epoch 0: : 32it [00:05,  5.78it/s, loss=0.769, v_num=02dh]Epoch 0: : 32it [00:05,  5.78it/s, loss=0.763, v_num=02dh]Epoch 0: : 33it [00:05,  5.94it/s, loss=0.763, v_num=02dh]Epoch 0: : 33it [00:05,  5.94it/s, loss=0.756, v_num=02dh]Epoch 0: : 34it [00:05,  6.05it/s, loss=0.756, v_num=02dh]Epoch 0: : 34it [00:05,  6.05it/s, loss=0.751, v_num=02dh]Epoch 0: : 35it [00:05,  6.20it/s, loss=0.751, v_num=02dh]Epoch 0: : 35it [00:05,  6.20it/s, loss=0.747, v_num=02dh]Epoch 0: : 36it [00:06,  5.88it/s, loss=0.747, v_num=02dh]Epoch 0: : 36it [00:06,  5.88it/s, loss=0.742, v_num=02dh]Epoch 0: : 37it [00:06,  6.03it/s, loss=0.742, v_num=02dh]Epoch 0: : 37it [00:06,  6.03it/s, loss=0.739, v_num=02dh]Epoch 0: : 38it [00:06,  6.15it/s, loss=0.739, v_num=02dh]Epoch 0: : 38it [00:06,  6.15it/s, loss=0.736, v_num=02dh]Epoch 0: : 39it [00:06,  6.29it/s, loss=0.736, v_num=02dh]Epoch 0: : 39it [00:06,  6.29it/s, loss=0.733, v_num=02dh]Epoch 0: : 40it [00:06,  5.94it/s, loss=0.733, v_num=02dh]Epoch 0: : 40it [00:06,  5.94it/s, loss=0.731, v_num=02dh]Epoch 0: : 41it [00:06,  6.07it/s, loss=0.731, v_num=02dh]Epoch 0: : 41it [00:06,  6.07it/s, loss=0.729, v_num=02dh]Epoch 0: : 42it [00:06,  6.20it/s, loss=0.729, v_num=02dh]Epoch 0: : 42it [00:06,  6.20it/s, loss=0.726, v_num=02dh]Epoch 0: : 43it [00:06,  6.34it/s, loss=0.726, v_num=02dh]Epoch 0: : 43it [00:06,  6.34it/s, loss=0.724, v_num=02dh]Epoch 0: : 44it [00:07,  6.05it/s, loss=0.724, v_num=02dh]Epoch 0: : 44it [00:07,  6.05it/s, loss=0.722, v_num=02dh]Epoch 0: : 45it [00:07,  6.17it/s, loss=0.722, v_num=02dh]Epoch 0: : 45it [00:07,  6.17it/s, loss=0.72, v_num=02dh] Epoch 0: : 46it [00:07,  6.29it/s, loss=0.72, v_num=02dh]Epoch 0: : 46it [00:07,  6.29it/s, loss=0.718, v_num=02dh]Epoch 0: : 47it [00:07,  6.42it/s, loss=0.718, v_num=02dh]Epoch 0: : 47it [00:07,  6.42it/s, loss=0.716, v_num=02dh]Epoch 0: : 48it [00:07,  6.09it/s, loss=0.716, v_num=02dh]Epoch 0: : 48it [00:07,  6.09it/s, loss=0.715, v_num=02dh]Epoch 0: : 49it [00:07,  6.21it/s, loss=0.715, v_num=02dh]Epoch 0: : 49it [00:07,  6.21it/s, loss=0.713, v_num=02dh]Epoch 0: : 50it [00:07,  6.32it/s, loss=0.713, v_num=02dh]Epoch 0: : 50it [00:07,  6.32it/s, loss=0.712, v_num=02dh]Epoch 0: : 51it [00:07,  6.44it/s, loss=0.712, v_num=02dh]Epoch 0: : 51it [00:07,  6.44it/s, loss=0.711, v_num=02dh]Epoch 0: : 52it [00:08,  6.10it/s, loss=0.711, v_num=02dh]Epoch 0: : 52it [00:08,  6.10it/s, loss=0.71, v_num=02dh] Epoch 0: : 53it [00:08,  6.21it/s, loss=0.71, v_num=02dh]Epoch 0: : 53it [00:08,  6.21it/s, loss=0.709, v_num=02dh]Epoch 0: : 54it [00:08,  6.32it/s, loss=0.709, v_num=02dh]Epoch 0: : 54it [00:08,  6.32it/s, loss=0.708, v_num=02dh]Epoch 0: : 55it [00:08,  6.42it/s, loss=0.708, v_num=02dh]Epoch 0: : 55it [00:08,  6.42it/s, loss=0.707, v_num=02dh]Epoch 0: : 56it [00:09,  6.18it/s, loss=0.707, v_num=02dh]Epoch 0: : 56it [00:09,  6.18it/s, loss=0.706, v_num=02dh]Epoch 0: : 57it [00:09,  6.28it/s, loss=0.706, v_num=02dh]Epoch 0: : 57it [00:09,  6.28it/s, loss=0.706, v_num=02dh]Epoch 0: : 58it [00:09,  6.39it/s, loss=0.706, v_num=02dh]Epoch 0: : 58it [00:09,  6.39it/s, loss=0.705, v_num=02dh]Epoch 0: : 59it [00:09,  6.49it/s, loss=0.705, v_num=02dh]Epoch 0: : 59it [00:09,  6.49it/s, loss=0.705, v_num=02dh]Epoch 0: : 60it [00:09,  6.20it/s, loss=0.705, v_num=02dh]Epoch 0: : 60it [00:09,  6.20it/s, loss=0.704, v_num=02dh]Epoch 0: : 61it [00:09,  6.30it/s, loss=0.704, v_num=02dh]Epoch 0: : 61it [00:09,  6.29it/s, loss=0.704, v_num=02dh]Epoch 0: : 62it [00:09,  6.38it/s, loss=0.704, v_num=02dh]Epoch 0: : 62it [00:09,  6.37it/s, loss=0.703, v_num=02dh]Epoch 0: : 63it [00:09,  6.47it/s, loss=0.703, v_num=02dh]Epoch 0: : 63it [00:09,  6.47it/s, loss=0.703, v_num=02dh]Epoch 0: : 64it [00:10,  6.27it/s, loss=0.703, v_num=02dh]Epoch 0: : 64it [00:10,  6.27it/s, loss=0.703, v_num=02dh]Epoch 0: : 65it [00:10,  6.36it/s, loss=0.703, v_num=02dh]Epoch 0: : 65it [00:10,  6.36it/s, loss=0.703, v_num=02dh]Epoch 0: : 66it [00:10,  6.38it/s, loss=0.703, v_num=02dh]Epoch 0: : 66it [00:10,  6.38it/s, loss=0.703, v_num=02dh]Epoch 0: : 67it [00:10,  6.46it/s, loss=0.703, v_num=02dh]Epoch 0: : 67it [00:10,  6.46it/s, loss=0.703, v_num=02dh]Epoch 0: : 68it [00:10,  6.34it/s, loss=0.703, v_num=02dh]Epoch 0: : 68it [00:10,  6.34it/s, loss=0.703, v_num=02dh]Epoch 0: : 69it [00:10,  6.43it/s, loss=0.703, v_num=02dh]Epoch 0: : 69it [00:10,  6.43it/s, loss=0.703, v_num=02dh]Epoch 0: : 70it [00:10,  6.42it/s, loss=0.703, v_num=02dh]Epoch 0: : 70it [00:10,  6.42it/s, loss=0.702, v_num=02dh]Epoch 0: : 71it [00:10,  6.50it/s, loss=0.702, v_num=02dh]Epoch 0: : 71it [00:10,  6.50it/s, loss=0.702, v_num=02dh]Epoch 0: : 72it [00:11,  6.38it/s, loss=0.702, v_num=02dh]Epoch 0: : 72it [00:11,  6.38it/s, loss=0.702, v_num=02dh]Epoch 0: : 73it [00:11,  6.46it/s, loss=0.702, v_num=02dh]Epoch 0: : 73it [00:11,  6.46it/s, loss=0.702, v_num=02dh]Epoch 0: : 74it [00:11,  6.46it/s, loss=0.702, v_num=02dh]Epoch 0: : 74it [00:11,  6.46it/s, loss=0.702, v_num=02dh]Epoch 0: : 75it [00:11,  6.54it/s, loss=0.702, v_num=02dh]Epoch 0: : 75it [00:11,  6.54it/s, loss=0.702, v_num=02dh]Epoch 0: : 76it [00:11,  6.38it/s, loss=0.702, v_num=02dh]Epoch 0: : 76it [00:11,  6.38it/s, loss=0.702, v_num=02dh]Epoch 0: : 77it [00:11,  6.46it/s, loss=0.702, v_num=02dh]Epoch 0: : 77it [00:11,  6.46it/s, loss=0.702, v_num=02dh]Epoch 0: : 78it [00:12,  6.50it/s, loss=0.702, v_num=02dh]Epoch 0: : 78it [00:12,  6.50it/s, loss=0.702, v_num=02dh]Epoch 0: : 79it [00:12,  6.57it/s, loss=0.702, v_num=02dh]Epoch 0: : 79it [00:12,  6.57it/s, loss=0.702, v_num=02dh]Epoch 0: : 80it [00:12,  6.44it/s, loss=0.702, v_num=02dh]Epoch 0: : 80it [00:12,  6.44it/s, loss=0.702, v_num=02dh]Epoch 0: : 81it [00:12,  6.51it/s, loss=0.702, v_num=02dh]Epoch 0: : 81it [00:12,  6.51it/s, loss=0.702, v_num=02dh]Epoch 0: : 82it [00:12,  6.55it/s, loss=0.702, v_num=02dh]Epoch 0: : 82it [00:12,  6.55it/s, loss=0.702, v_num=02dh]Epoch 0: : 83it [00:12,  6.62it/s, loss=0.702, v_num=02dh]Epoch 0: : 83it [00:12,  6.62it/s, loss=0.702, v_num=02dh]Epoch 0: : 84it [00:12,  6.49it/s, loss=0.702, v_num=02dh]Epoch 0: : 84it [00:12,  6.48it/s, loss=0.702, v_num=02dh]Epoch 0: : 85it [00:12,  6.55it/s, loss=0.702, v_num=02dh]Epoch 0: : 85it [00:12,  6.55it/s, loss=0.701, v_num=02dh]Epoch 0: : 86it [00:13,  6.59it/s, loss=0.701, v_num=02dh]Epoch 0: : 86it [00:13,  6.59it/s, loss=0.701, v_num=02dh]Epoch 0: : 87it [00:13,  6.66it/s, loss=0.701, v_num=02dh]Epoch 0: : 87it [00:13,  6.66it/s, loss=0.701, v_num=02dh]Epoch 0: : 88it [00:13,  6.54it/s, loss=0.701, v_num=02dh]Epoch 0: : 88it [00:13,  6.54it/s, loss=0.701, v_num=02dh]Epoch 0: : 89it [00:13,  6.60it/s, loss=0.701, v_num=02dh]Epoch 0: : 89it [00:13,  6.60it/s, loss=0.701, v_num=02dh]Epoch 0: : 90it [00:13,  6.64it/s, loss=0.701, v_num=02dh]Epoch 0: : 90it [00:13,  6.64it/s, loss=0.701, v_num=02dh]Epoch 0: : 91it [00:13,  6.70it/s, loss=0.701, v_num=02dh]Epoch 0: : 91it [00:13,  6.70it/s, loss=0.701, v_num=02dh]Epoch 0: : 92it [00:14,  6.54it/s, loss=0.701, v_num=02dh]Epoch 0: : 92it [00:14,  6.54it/s, loss=0.701, v_num=02dh]Epoch 0: : 93it [00:14,  6.60it/s, loss=0.701, v_num=02dh]Epoch 0: : 93it [00:14,  6.60it/s, loss=0.701, v_num=02dh]Epoch 0: : 94it [00:14,  6.64it/s, loss=0.701, v_num=02dh]Epoch 0: : 94it [00:14,  6.64it/s, loss=0.701, v_num=02dh]Epoch 0: : 95it [00:14,  6.71it/s, loss=0.701, v_num=02dh]Epoch 0: : 95it [00:14,  6.71it/s, loss=0.701, v_num=02dh]Epoch 0: : 96it [00:14,  6.54it/s, loss=0.701, v_num=02dh]Epoch 0: : 96it [00:14,  6.54it/s, loss=0.701, v_num=02dh]Epoch 0: : 97it [00:14,  6.60it/s, loss=0.701, v_num=02dh]Epoch 0: : 97it [00:14,  6.60it/s, loss=0.701, v_num=02dh]Epoch 0: : 98it [00:14,  6.65it/s, loss=0.701, v_num=02dh]Epoch 0: : 98it [00:14,  6.65it/s, loss=0.7, v_num=02dh]  Epoch 0: : 99it [00:14,  6.71it/s, loss=0.7, v_num=02dh]Epoch 0: : 99it [00:14,  6.71it/s, loss=0.7, v_num=02dh]Epoch 0: : 100it [00:15,  6.57it/s, loss=0.7, v_num=02dh]Epoch 0: : 100it [00:15,  6.56it/s, loss=0.7, v_num=02dh]Error executing job with overrides: ['training.accelerator=gpu', 'project.wandb_mode=run']
Traceback (most recent call last):
  File "/faststorage/project/NLPPred/github/snip/src/snip/train_slided_autoencoder.py", line 211, in main
    trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 696, in fit
    self._call_and_handle_interrupt(
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 650, in _call_and_handle_interrupt
    return trainer_fn(*args, **kwargs)
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 735, in _fit_impl
    results = self._run(model, ckpt_path=self.ckpt_path)
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1166, in _run
    results = self._run_stage()
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1252, in _run_stage
    return self._run_train()
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1283, in _run_train
    self.fit_loop.run()
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/loops/loop.py", line 201, in run
    self.on_advance_end()
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/loops/fit_loop.py", line 299, in on_advance_end
    self.trainer._call_callback_hooks("on_train_epoch_end")
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py", line 1597, in _call_callback_hooks
    fn(self, self.lightning_module, *args, **kwargs)
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 183, in on_train_epoch_end
    self._run_early_stopping_check(trainer)
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 194, in _run_early_stopping_check
    if trainer.fast_dev_run or not self._validate_condition_metric(  # disable early_stopping with fast_dev_run
  File "/home/kce/miniconda3/envs/snip2/lib/python3.9/site-packages/pytorch_lightning/callbacks/early_stopping.py", line 149, in _validate_condition_metric
    raise RuntimeError(error_msg)
RuntimeError: Early stopping conditioned on metric `Validation loss` which is not available. Pass in or modify your `EarlyStopping` callback to use any of the following: `Training loss`, `Training step/sec`

Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.
wandb: Waiting for W&B process to finish... (failed 1). Press Control-C to abort syncing.
wandb: - 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: / 0.001 MB of 0.001 MB uploaded (0.000 MB deduped)wandb: - 0.001 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: \ 0.001 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.001 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.003 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: - 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: \ 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: - 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: \ 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: | 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb: / 0.017 MB of 0.017 MB uploaded (0.000 MB deduped)wandb:                                                                                
wandb: 
wandb: Run history:
wandb:        Model number ‚ñÅ
wandb:            N models ‚ñÅ
wandb:       Training loss ‚ñà‚ñÉ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb:   Training step/sec ‚ñÉ‚ñà‚ñà‚ñá‚ñÅ‚ñá‚ñà‚ñÜ‚ñÜ‚ñà
wandb:               epoch ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ
wandb: trainer/global_step ‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñÑ‚ñÖ‚ñÜ‚ñÜ‚ñá‚ñà
wandb: 
wandb: Run summary:
wandb:        Model number 0
wandb:            N models 19
wandb:       Training loss 0.69924
wandb:   Training step/sec 0.00212
wandb:               epoch 0
wandb: trainer/global_step 99
wandb: 
wandb: Synced desert-feather-24: https://wandb.ai/kenevoldsen/local%20mlp/runs/3qap02dh
wandb: Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./wandb/run-20221002_025630-3qap02dh/logs
Epoch 0: : 100it [00:21,  4.55it/s, loss=0.7, v_num=02dh]