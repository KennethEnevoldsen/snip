project:
  name: "local mlp"
  wandb_mode: "dryrun" # Which mode to run WandB in. Takes "run", "dryrun", "offline" and "disabled"
  seed: 42
  run_name_prefix: "test_" # str: prefix of wandb run name
  verbose: true # bool

data:
  test_path: null # Optional[str]
  train_path: "tests/data/sample_train.zarr"
  validation_path: "tests/data/sample_validation.zarr"
  result_path: "tests/data/processed/"
  interim_path: "tests/data/interim/" # temporary data path
  # training data args
  stride: 100 # if stride>input then there is an overlap
  width: 100 # int: number of snps. Inpu should be equal to the input layer. Note: should be equal to model input_size
  batch_size: 100 # int
  impute_missing: "mean"  # Optional[str]: method for imputing missing snps.

training:
  # pytorch lightning trainer args:
  accelerator: "cpu" # "mps" | "gpu" | "cpu"
  devices: 1 # int: number of devices
  log_step: 10 # int
  precision: 32 # 64 | 32 | 16 | "bf16"
  default_root_dir: "test/models/"
  profiler: null
  val_check_interval: null # Optional[int]: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=val_check_interval#pytorch_lightning.trainer.Trainer.params.val_check_interval
  max_epochs: null # Optional[int]: Number of epochs to train
  auto_lr_find: true # bool: https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=val_check_interval#pytorch_lightning.trainer.Trainer.params.auto_lr_find
  patience: 10, # Optional[int]: early stopping patience
  check_val_every_n_epoch: null # Optional[int] https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html?highlight=val_check_interval#pytorch_lightning.trainer.Trainer.params.check_val_every_n_epoch
  # other args
  optimizer: "adam" # str:
  learning_rate: 0.001 # float: will be ignored if auto_lr_find is true.

model:
  input_size: 100 # int
  encode_layers: [75, 50]
  decode_layers: [75, 100]
  activation: "linear" # str: Options include linear and relu
